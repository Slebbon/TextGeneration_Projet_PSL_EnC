# Shrump 
Finetuning of different large language models on a mixed dataset of Shakespeare quotes and Trump tweets

<br>

## The project
The goal of the project is to finetune some language models to make them generate text in a mixed style of Trump and Shakespeare. We did this on three different models: GPT-2, FLAN-T5 and Mistral-7b. Next, we measured the performance of the different models through a BERT text classifier applied to the generated texts. 

<br>

## Data

The final dataset used for training comes from the union of a dataset on Shakespeare and one on Trump. <br><br>
For Shakespeare, the data were taken from [here](https://www.kaggle.com/datasets/kingburrito666/shakespeare-plays) <br>
For Trump, the data were taken from [here](https://www.kaggle.com/datasets/kingburrito666/better-donald-trump-tweets/data).

The data were merged and balanced according to the number of tokens present for the two authors. In this way, at the end in the balanced dataset each author has about the same number of tokens. The code used to do that is in `Dataset/Dataset_code/Data_Balancing.ipynb`. The final dataset has two columns, one with the text and one with the author (Shakespeare or Trump).

The balanced dataset was saved in the `Divided_Dataset` folder by splitting it into the canonical "train, test, validation" division. <br><br>
Also in the same folder are the files `I_train.csv`, `I_test.csv` and `I_validation.csv`. These were created with the code in `Dataset/Dataset_code/Dataset_Integration.ipynb`. This is the dataset used to train a BERT classifier, in order to classify a text according to its author: Trump, Shakespeare, Other. These files therefore contain quotes from Trump, Shakespeare and other authors. These were taken from [this dataset](https://huggingface.co/datasets/sentence-transformers/parallel-sentences-news-commentary) on english news.


<br>

## Training

Due to computational limitations, we could not guarantee consistent training of the various models in terms of hyperparameters. 

GPT-2, for example, was trained on 3 epochs using the total training data.

Mistral-7b, on the other hand, was trained on only 1 epoch and 10% of the total training data. 

FLAN was trained on 3 epoch on 10% of the total training data.

## Performance evaluation

To measure the performance of the models after finetuning, we trained a BERT text classifier on a set of texts by Trump, Shakespeare and various authors. At the end of the training, the classifier was able to label a text according to its author: "Shakespeare", "Trump", "Other". The model was trained on a dataset of about 32 thousand text excerpts over three epochs. Below are the metrics after training. 

<p align="center">
<img src="./images/classifier_metrics.png" alt="drawing" width="500"/>
</p>

We then generated a series of texts using the various models-both finetuned and basic versions. The prompts we used were taken from the multitask evaluation dataset created in
<i>Measuring Massive Multitask Language Understanding</i> by Dan Hendrycks et al. (ICLR 2021). We used only 10% of the prompts, i.e. 154 prompts of very different topics. 
<br><br>

Once the responses were generated with the various models, we applied the classifier to see the effects of finetuning. Ideally, the classifier should have identified Trump or Shakespeare as the authors of most of the texts generated by the finetuned models.

This was the case for GPT-2. These are the results of the performance test:


<p align="center">
<img src="./images/performance_GPT2.png" alt="drawing" width="500"/>
</p>

Trump was recognized as the author of 93% of the texts generated by the finetuned model, while it was only 24% for the base model. 

<br><br>

Mistral-7b finetuning's performance was not as good. 

<p align="center">
<img src="./images/performance_mistral.png" alt="drawing" width="500"/>
</p>

"Other" is labeled as the author of more than half the texts. Paradoxically, the scores of the finetuned model are even worse than the basic model.

<br><br>

## Content

- `Dataset`: code and data used to create the final datasets

    - `finetuning_dataset`: final dataset used to finetune the models

    - `integrated_dataset`: dataset used to train the BERT classifier for performance evaluation

- `Finetuning_notebooks`: notebooks used to finetune the three models

- `performance_evaluation`: code used to finetune the BERT classifier and code used to evaluate the performance of the models through the classifier