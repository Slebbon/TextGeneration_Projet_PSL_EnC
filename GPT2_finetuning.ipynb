{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Slebbon/TextGeneration_Projet_PSL_EnC/blob/main/GPT2_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM_74XlayElf",
        "outputId": "35090bfc-0fff-4698-ec39-94f6aeb2145d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oxyX7FN_x8fo"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpw3n0KKx8fp"
      },
      "source": [
        "  ## Parameters & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "s7fNb_PDx8fq"
      },
      "outputs": [],
      "source": [
        "pretrained_model = \"distilbert/distilgpt2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyaLcrDkx8fr",
        "outputId": "9239d838-982f-4111-cac2-69f7330da41f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available for notebook\n",
            "GPU Memory cleaned\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#setup parameters regarding GPU availibility on the machine and recycle used memory\n",
        "import torch;\n",
        "import gc;\n",
        "\n",
        "is_gpu_available = torch.cuda.is_available()\n",
        "device = 'cuda' if is_gpu_available else 'cpu'\n",
        "if is_gpu_available:\n",
        "    print(\"GPU available for notebook\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU Memory cleaned\")\n",
        "else:\n",
        "    print(\"No GPU available for notebook\")\n",
        "\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjFMEpoyx8fr"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49UYmw9ox8fr",
        "outputId": "4c51b8da-f5b3-44f1-974a-2d30cdfc1d07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Text', 'Author'],\n",
            "        num_rows: 21054\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['Text', 'Author'],\n",
            "        num_rows: 4513\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['Text', 'Author'],\n",
            "        num_rows: 4512\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"/content/\", data_files={\n",
        "    'train': f'train.csv',\n",
        "    'validation': f'validation.csv',\n",
        "    'test': f'test.csv'\n",
        "})\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EpJxX0tx8fs"
      },
      "source": [
        "  ## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpht-5ABx8fs",
        "outputId": "7561d2fa-5f1c-4f49-9d07-89cbb07963f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum context size: 1024\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(pretrained_model)\n",
        "# Get the maximum context size\n",
        "max_length = model.config.max_position_embeddings\n",
        "print(f\"Maximum context size: {max_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get % of data"
      ],
      "metadata": {
        "id": "FY6CH4y_6WSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_10 = dataset['train'].train_test_split(test_size=0.50)['test']\n",
        "dataset['train'] = train_10"
      ],
      "metadata": {
        "id": "NcrL8Mbi-5Y5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_10 = dataset['validation'].train_test_split(test_size=0.50)['test']\n",
        "dataset['validation'] = valid_10"
      ],
      "metadata": {
        "id": "4sKtuNkL9nZG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_10 = dataset['test'].train_test_split(test_size=0.50)['test']\n",
        "dataset['test'] = test_10"
      ],
      "metadata": {
        "id": "RuprHy-ZArsG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize"
      ],
      "metadata": {
        "id": "y0VL_LG66a9V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0b3cpw1Lx8fs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "82deb6c09240446d8d4987be4c9df964",
            "7b00754143684a00ace825852d2e6723",
            "845c934a7c06448ca89c82b6cbbf1157",
            "e83fe08830bc404e97e346ef9423d446",
            "6774246412294fd683e7554da219adee",
            "4c785901ac52497b9495fa62aaae6ddc",
            "d2bbd2d58d294f4db34d743f7d6a890d",
            "07ddc1a0aee34b718352e6340e80ee5b",
            "53b325de2544448aa67d0f6a97e134f8",
            "49ad9048db2b4f4ba72b4ec59efc529b",
            "6046a03be9604434b7665df6384113c1"
          ]
        },
        "outputId": "6ae607fd-2283-4f37-b89d-5d6debaa888c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4513 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82deb6c09240446d8d4987be4c9df964"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"Text\"],max_length=max_length)\n",
        "\n",
        "\n",
        "# Apply the tokenization function to the entire dataset\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=10,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cL8VfPtx8fs",
        "outputId": "e81b694d-2d4a-49b7-b2aa-88761f133f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18947, 890, 290, 13983, 13]\n"
          ]
        }
      ],
      "source": [
        "sample_token = tokenizer.encode(\"Live long and prosper.\")\n",
        "print(sample_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcX785fCx8fs"
      },
      "source": [
        "## Data Collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7YFj1RFx8ft",
        "outputId": "4f875ab0-4dd8-475e-fcd6-812dbfa78129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids shape: torch.Size([1, 9])\n",
            "attention_mask shape: torch.Size([1, 9])\n",
            "labels shape: torch.Size([1, 9])\n"
          ]
        }
      ],
      "source": [
        "#We need to create data collator to manage the batches, we can use DataCollatorForLanguageModeling\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "tokenizer.pad_token = \"<pad>\"\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer,mlm=False)\n",
        "# Iterate over the generator\n",
        "out = data_collator([tokenized_dataset[\"train\"][i] for i in range(1)])\n",
        "for key in out:\n",
        "    print(f\"{key} shape: {out[key].shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnA0aArGx8ft"
      },
      "source": [
        "## Setup the Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "d3c4sn12x8ft"
      },
      "outputs": [],
      "source": [
        "#Now we train the model using the Trainer API\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    'outputs',\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    learning_rate=2e-3,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=is_gpu_available,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    load_best_model_at_end=True,\n",
        "    save_steps = 500,\n",
        "    eval_steps=500\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvvlHP1tx8ft"
      },
      "source": [
        "## Evaluate the Performance of the Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "Ty_xyD6zx8ft",
        "outputId": "dcddb5ec-1d82-4525-fafa-520eda1e1342"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='565' max='565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [565/565 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 5.541857719421387, 'eval_runtime': 8.539, 'eval_samples_per_second': 528.516, 'eval_steps_per_second': 66.167}\n",
            "Baseline distilbert/distilgpt2 Results: Perplexity: 255.15\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "#Calculate and report on perplexity\n",
        "initial_results = trainer.evaluate()\n",
        "print(initial_results)\n",
        "#log the results to file\n",
        "#logger.info(f\"Baseline {pretrained_model} Results: Perplexity: {math.exp(initial_results['eval_loss']):.2f}\")\n",
        "print(f\"Baseline {pretrained_model} Results: Perplexity: {math.exp(initial_results['eval_loss']):.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AZ8jW7STx8ft"
      },
      "outputs": [],
      "source": [
        "#setup our test prompts\n",
        "test_prompt = \"What is the meaning of life?\"\n",
        "test_prompt2 = \"Where did that planet go??\"\n",
        "test_prompt3 = \"What is the best way to cook a turkey?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txulZuSTx8ft",
        "outputId": "8d553e5f-5474-482f-fa9f-cec65147a834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline distilbert/distilgpt2 generated result: What is the meaning of life?...What is the meaning of life? What you experience that way as a child? What you experience in your life is what you experience on a regular basis? (One important aspect of this study is what is most interesting is how you see the relationship between life and gender)\n",
            "\n",
            "\n",
            "The researchers from the National Institute of Standards and Technology study. They are now studying the effects of the gender binary at birth, as well as in many key studies from around the world. The researchers used a set of\n"
          ]
        }
      ],
      "source": [
        "#Use the model in a pipeline to generate text.\n",
        "from transformers import pipeline\n",
        "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
        "\n",
        "\n",
        "result = text_generator(test_prompt, max_length=100, num_return_sequences=1,temperature=1)\n",
        "print(f\"Baseline {pretrained_model} generated result: {test_prompt}...{result[0]['generated_text']}\")\n",
        "#logger.info(f\"Baseline {pretrained_model} generated result: {test_prompt}...{result[0]['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1XjWpxEx8fu",
        "outputId": "c4a00af3-7dfe-4eb3-ae45-b0e007b41183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline distilbert/distilgpt2 generated result: Where did that planet go??...Where did that planet go??????\n",
            "In fact, all of the stars in our universe are orbiting a few kilometers away, and there are more than 100 million planets across your Universe today. It is a mystery whether you‿d do it because you didn‿d know how you did it, or was it not planned to, but to, and to, and especially, your entire life. If so, what do you do now?\n",
            "In order to tell the truth, here\n"
          ]
        }
      ],
      "source": [
        "result = text_generator(test_prompt2, max_length=100, num_return_sequences=1,temperature=1)\n",
        "print(f\"Baseline {pretrained_model} generated result: {test_prompt2}...{result[0]['generated_text']}\")\n",
        "#logger.info(f\"Baseline {pretrained_model} generated result: {test_prompt2}...{result[0]['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y81z933Ex8fu",
        "outputId": "b40a3d14-88d7-4509-f0c5-ec648c1e7846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline distilbert/distilgpt2 generated result: What is the best way to cook a turkey?...What is the best way to cook a turkey? I really want things to be the best quality that you can find. So, I used to make turkey at Chipotle, but it‬s pretty slow, so it‬s really easy to do it in a traditional way. If this ain't been the case then it is definitely a little different. I did this for Thanksgiving and hope my turkey stays where I am today that I'm going for it, so take it home if you\n"
          ]
        }
      ],
      "source": [
        "result = text_generator(test_prompt3, max_length=100, num_return_sequences=1,temperature=1)\n",
        "print(f\"Baseline {pretrained_model} generated result: {test_prompt3}...{result[0]['generated_text']}\")\n",
        "#logger.info(f\"Baseline {pretrained_model} generated result: {test_prompt3}...{result[0]['generated_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC545Cq6x8fu"
      },
      "source": [
        "## Fine-Tune the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fdWFA5sFx8fu",
        "outputId": "5470df4b-4699-47d6-af29-512a72496c73"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7896' max='7896' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7896/7896 12:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>5.997900</td>\n",
              "      <td>5.591518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>5.511400</td>\n",
              "      <td>5.370150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.329000</td>\n",
              "      <td>5.199179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>5.143800</td>\n",
              "      <td>5.054142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>5.028800</td>\n",
              "      <td>4.977328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>4.570500</td>\n",
              "      <td>5.018639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>4.414300</td>\n",
              "      <td>4.991149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>4.383300</td>\n",
              "      <td>4.871944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>4.339700</td>\n",
              "      <td>4.834546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>4.307600</td>\n",
              "      <td>4.777462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>3.892600</td>\n",
              "      <td>5.060309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>3.448500</td>\n",
              "      <td>5.021846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>3.464600</td>\n",
              "      <td>5.009911</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>3.402200</td>\n",
              "      <td>4.991721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>3.360500</td>\n",
              "      <td>4.992687</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1130' max='565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [565/565 01:19]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n",
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7896, training_loss=4.385359527371454, metrics={'train_runtime': 751.3917, 'train_samples_per_second': 84.06, 'train_steps_per_second': 10.509, 'total_flos': 715670365372416.0, 'train_loss': 4.385359527371454, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi44PoyUx8fu"
      },
      "source": [
        "## Evaluate the Performance of the Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "xjZedhx9x8fu",
        "outputId": "78c63302-dda7-4cf2-d17d-3ae2552e4e95"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='565' max='565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [565/565 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned results: Perplexity: 118.80\n"
          ]
        }
      ],
      "source": [
        "#Calculate and report on perplexity\n",
        "eval_results = trainer.evaluate()\n",
        "perplexity = math.exp(eval_results['eval_loss'])\n",
        "eval_results['perplexity'] = perplexity\n",
        "\n",
        "#logger.info(f\"Fine-tuned {finetuned_modelname} Results: Perplexity: {perplexity:.2f}\")\n",
        "print(f\"Fine-tuned results: Perplexity: {perplexity:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdwPQHbwFTEr",
        "outputId": "e9c3c64f-1cc5-4f12-dbc8-f138e4aca9c9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 4.777462005615234,\n",
              " 'eval_runtime': 7.6922,\n",
              " 'eval_samples_per_second': 586.697,\n",
              " 'eval_steps_per_second': 73.451,\n",
              " 'epoch': 3.0,\n",
              " 'perplexity': 118.8024471491827}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaQn49pLx8fu",
        "outputId": "c2530299-04fb-40cf-8b0c-19826633c82c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned generated result: What is the meaning of life?...What is the meaning of life? What's that. Please you? What's there! Come, look, here? look ye? no? no? I'll rail! we're a true! My friend, I do. You? What's a fool! I'll not good answer? O, then? how you. Now, nor a fool, there these? If the lie? Good. But, you know, by a good. What we? If it's anon's we's a man. What's not sure? What a word? What we'll I mean? I think so. But when when she's a lie? and the devil's the horse? no, I will we'll tell? How dost thou art thou art here's the matter to the gentleman? I said he's, and we can't take this, the letter? look you. You must I do us there's that's there's not? I think, thou'ld stand?\n"
          ]
        }
      ],
      "source": [
        "#Prompt Test 1\n",
        "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
        "result = text_generator(test_prompt, max_length=200, num_return_sequences=1,temperature=1)\n",
        "print(f\"Fine-tuned generated result: {test_prompt}...{result[0]['generated_text']}\")\n",
        "#logger.info(f\"Fine-tuned {finetuned_modelname} generated result: {test_prompt}...{result[0]['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPUdgr7Rx8fu",
        "outputId": "96b11607-fefa-4128-9796-fb5c3f09db5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned generated result: Where did that planet go??...Where did that planet go?? Now at the bottom of this hour hence. What's that? I do? look how they are? Why? Nothing's here? How? I, she? Let's not?'s?\" See. So, let my wife? oh me good!\n",
            "@HillaryClinton!_Good, so. You're with all. So sad, you look? I think you see you're doing that? We're doing nothing, what they looked they have the thing, a great state? Why do? She? I think she was she was a lot of it! What a bad judgement see that. Good morrow, I said you see, and there was a little bit of time in November 8 points from thence, sir, they're rebuilding the media, sir a lot of your new Washington Times/them, when if ever to have said I said what I can it, and I wouldn't be talking very nice girl, she can she said it's\n"
          ]
        }
      ],
      "source": [
        "#Prompt Test 2\n",
        "result = text_generator(test_prompt2, max_length=200, num_return_sequences=1,temperature=1)\n",
        "print(f\"Fine-tuned generated result: {test_prompt2}...{result[0]['generated_text']}\")\n",
        "#logger.info(f\"Fine-tuned {finetuned_modelname} generated result: {test_prompt2}...{result[0]['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbR-kZJ9x8fu",
        "outputId": "68aefd85-e391-41ff-d18b-f81d796d37b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned generated result: What is the best way to cook a turkey?...What is the best way to cook a turkey? O you great spirit, there! No problem! https://t.co/ZXJlN4K2GxMv1wjA5S0. He did not come better look so proud??\" So true! He would not to fix it. https://t.co/lNZtwZgVb\"A\"He6eV1yYt.2MtCWRy6KHLVT1jZl3W\" So great to do.2K2y9\"@s0D\"  @politico? #MakeAmericaGreatAgain\" https://t.JjW\"  https://t.cooper at the beginningZl2Bj1yH3uMq1w9LqD2t.co/ZyD0Dms2Xl3lWjrqRrP9l4\n"
          ]
        }
      ],
      "source": [
        "#Prompt Test 3\n",
        "result = text_generator(test_prompt3, max_length=200, num_return_sequences=1,temperature=1)\n",
        "print(f\"Fine-tuned generated result: {test_prompt3}...{result[0]['generated_text']}\")\n",
        "#logger.info(f\"Fine-tuned {finetuned_modelname} generated result: {test_prompt3}...{result[0]['generated_text']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import mkdir\n",
        "mkdir('/content/final_model')"
      ],
      "metadata": {
        "id": "iaSS5oheKKIR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('/content/final_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjTgwU5sE4Vb",
        "outputId": "98dc5c9e-bf25-4441-a01e-3162a9aa80a1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 50, 'do_sample': True}\n",
            "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C54x5HrALPFW",
        "outputId": "b3763d09-a44a-4f2a-ca0f-148dee5785db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/final_model /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "WVABp5g6LPoj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test model"
      ],
      "metadata": {
        "id": "ntPOdFa9Ni_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained('/content/final_model')"
      ],
      "metadata": {
        "id": "WL7KsP56NoCb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('/content/final_model')"
      ],
      "metadata": {
        "id": "DSjYijdtNySO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ftyOXA1_ORV-",
        "outputId": "ce218f5c-ceb7-4225-9e36-997fe54684a7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<pad>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "S_NkUW5fLz5m"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_generator('Hi, how are you?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LljDk1-vN8_-",
        "outputId": "a0e20411-aa48-4e87-8f43-a45433de6c50"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1359: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Hi, how are you? Sir Toby, tell me how I can. The same, that you do! I hope, it will be too long. Thank you! #NewYorkValues #Trump2016\\nRegisterT_ https://t.co'}]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_generator('What are you doing?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySOEFCTEOAPv",
        "outputId": "071e06d0-55f3-4cf7-e9fa-80ebc3e2e579"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'What are you doing? I mean but they are going. They should not give them gold, but by our new, in my bed--so? It shall be too far more! Thanks. @MichaelCohen212! https://t.co'}]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_generator('To be or not to be')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4eY1TAPOIbk",
        "outputId": "58cf1d2d-9a95-49f7-c7ed-bcbe9f146be3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"To be or not to be denied yourself, but, nor the king's a fool. When dinner comes. O! you too. I love me. Good Lord Timon. Now to have no credibility!    I love them.' What\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_generator('Make America')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_QNWN8aOPNm",
        "outputId": "f498cc38-7711-401e-bf59-791e006475f0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Make America great again! Thank you. Thank you. #AmericaFirst https://t.co/3YDZWOl2O http://t.co/yG4KEzbQq https://t.co/e'}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v1N1UyZZOZ2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "36b514b738acff4061330311a03dff86b1232f3a413278e3b727ffbc7c44120f"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "82deb6c09240446d8d4987be4c9df964": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b00754143684a00ace825852d2e6723",
              "IPY_MODEL_845c934a7c06448ca89c82b6cbbf1157",
              "IPY_MODEL_e83fe08830bc404e97e346ef9423d446"
            ],
            "layout": "IPY_MODEL_6774246412294fd683e7554da219adee"
          }
        },
        "7b00754143684a00ace825852d2e6723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c785901ac52497b9495fa62aaae6ddc",
            "placeholder": "​",
            "style": "IPY_MODEL_d2bbd2d58d294f4db34d743f7d6a890d",
            "value": "Map: 100%"
          }
        },
        "845c934a7c06448ca89c82b6cbbf1157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07ddc1a0aee34b718352e6340e80ee5b",
            "max": 4513,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53b325de2544448aa67d0f6a97e134f8",
            "value": 4513
          }
        },
        "e83fe08830bc404e97e346ef9423d446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49ad9048db2b4f4ba72b4ec59efc529b",
            "placeholder": "​",
            "style": "IPY_MODEL_6046a03be9604434b7665df6384113c1",
            "value": " 4513/4513 [00:01&lt;00:00, 3918.01 examples/s]"
          }
        },
        "6774246412294fd683e7554da219adee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c785901ac52497b9495fa62aaae6ddc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2bbd2d58d294f4db34d743f7d6a890d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07ddc1a0aee34b718352e6340e80ee5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53b325de2544448aa67d0f6a97e134f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49ad9048db2b4f4ba72b4ec59efc529b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6046a03be9604434b7665df6384113c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}